<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Making News Go Viral</title>
    <meta charset="utf-8" />
    <meta name="author" content="Cin Cin Fang &amp; Michael Leibert" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="test.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Making News Go Viral
## Math 640 Class Project Presentation
### Cin Cin Fang &amp; Michael Leibert
### (updated: 2019-05-09)

---




#Data


Online News Popularity Data Set 

 + This dataset contains rougly 45 attributes about articles published by Mashable in a period of two years. The goal is to predict the number of shares in social networks (popularity).
 
 + Variables range from category (business, entertainment, lifestyle, etc), measures of the article's subjectivity and polarity, and the average length words.
 
 + We end up removing some variables that display multicollinearity, and we also scale the continuous variables.

 + Dataset has 39,644 samples. We will use a 30,000 train / 9,644 test split.
---

#Data


A sample of a few rows and columns:


```
##                                                       url num_imgs
##  http://mashable.com/2013/07/03/lion-forge-80s-tv-comics/        6
##           http://mashable.com/2013/07/03/low-cost-iphone/       15
##          http://mashable.com/2013/07/03/mediashift-wi-fi/        1
##                                                                   
##  num_videos global_subjectivity max_negative_polarity topic shares
##           0         0.421512501                -0.025  tech    792
##           1         0.503344852                 -0.05 other 843300
##           0         0.301536797                  -0.1 world   9500
## 
```



---
# Goals


&lt;br&gt;

&lt;br&gt;

&lt;br&gt;


 + The goal is to find the attributes that can predict how sharable an article is.


--


 + We will utilize the Bayesian ordinal probit model.


--

 + We will test the robustness of our estimates for `\(\beta\)` with three different priors on `\(\beta\)`: a flat prior, a Laplacian prior, and a Normal prior.

--

 + We will out-of-sample testing to check our results.


---
# Methods

 + Rather than attempt to predict the given "shares" directly, we instead translate "shares" of the articles `\(i = 1,...,n\)` into quintiles to form ordinal categories.

--

 + Again, this lends itself to ordinal probit regression, which can be more easily modeled in a Bayesian context through latent variables.



--

 + For this formulation, the latent variable `\(z_i\)` of article `\(i\)`, `\(z \overset{iid}{\sim} N( x'_i \beta ,1)\)`

 + Where `\(x_i\)` is the vector of predictive variables and `\(\beta\)` is the vector of coeffcients we are ultimately interested in.
 
--

The observed response variable `\(y_i\)`, which quantile of shares does it attract, is:


`$$y_i = \begin{cases} \hfil 1 &amp;  \text{if  } \  \gamma_0 &lt; z_i \leq \gamma_1 \\[.3 em] \hfil 2  &amp; \text{if  } \ \gamma_1 &lt; z_i \leq \gamma_2 \\[.3 em] &amp; \hspace{11 mm} \vdots \\[.4 em] \hfil 5  &amp; \text{if  } \ \gamma_4 &lt; z_i \leq \gamma_5 \\ \end{cases}$$`


Where `\(\gamma_0\)` is `\(-\infty\)`, `\(\gamma_1\)` is fixed at 0, and `\(\gamma_5\)` is `\(\infty\)`

---

# Flat Prior

&lt;br&gt;

Likelihood



`$$\begin{align*} \mathcal{L}(y|z,\gamma,\beta,X) &amp; \propto \prod_{i=1}^{n} \left[ \ \sum_{j=1}^5 I_{ \{ \gamma_{j-1} &lt; z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z_i - x'_i \beta \right)^2 \right] \\[.5em] &amp; \propto  \left[ \ \prod_{i=1}^{n} \sum_{j=1}^5 I_{ \{ \gamma_{j-1} &lt; z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z-X\beta\right)'\left(z-X\beta\right) \right] \\[1em] \end{align*}$$`


&lt;br&gt;

--

Under the flat prior `\(\pi(\beta)\propto 1\)`, the posterior is unchanged.  

&lt;br&gt;

`$$\begin{align*} P(z,\gamma,\beta|y,X) &amp;  \propto  \left[ \ \prod_{i=1}^{n} \sum_{j=1}^5 I_{ \{ \gamma_{j-1} &lt; z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z-X\beta\right)'\left(z-X\beta\right) \right] \end{align*}$$` 


---

# Flat Prior

We now derive the conditional for `\(\beta\)`:

`$$\begin{align*} p(\beta|z,\gamma,X,y) &amp; \propto \exp\left[-\cfrac{1}{2} \left( \beta'X'X\beta - 2\beta'X'X(X'X)^{-1}X'z \right) \right]\end{align*}$$`

--

Which is the kernel of a normal distribution, so `$$\begin{align*} \beta|z,\gamma,X,y \sim N\Big( (X'X)^{-1}X'z , (X'X)^{-1}  \Big)\end{align*}$$`

&lt;br&gt;
--

The conditional for `\(\gamma_j\)`, where `\(j=2,3,4\)`:

`$$\begin{align*} p(\gamma_j|z,\beta,X,y) &amp; \propto \prod_{i=1}^n I_{ \{ \gamma_{j-1} &lt; z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}} + I_{ \{ \gamma_{j} &lt; z_i \leq \gamma_{j+1} \} }  \ I_{\{y_i = j + 1 \}}\end{align*}$$`
--

Which is a uniform distribution

`$$\gamma_j|z,\beta,X,y \sim U\Big(\max\{\max\{z_i: y_i = j\}, \gamma_{j-1} \}, \min\{\min\{ z_i:y_i = j+1\}, \gamma_{j+1}\} \Big)$$`


---

#Flat Prior


&lt;br&gt;

&lt;br&gt;

To find the conditionals for `\(z_i\)`, we must consider the separate cases of `\(y_i\)`

--

If `\(y_i = j\)`, the conditional for `\(z_i\)` is:


`$$\begin{align*} p( z |\gamma,\beta,X,y) &amp; \propto  \exp\left[-\cfrac{1}{2}\left(z_i - x'_i\beta \right)^2\right]\end{align*}  I_{ \{ \gamma_{j-1} &lt; z_i \leq \gamma_{j} \} }$$`


Which is a truncated normal with mean `\(x'_i\beta\)`, variance 1, and truncated to be between `\(\gamma_{j-1}\)` and `\(\gamma_j\)`.




---

# Penalization

&lt;br&gt;

 + With a large number of predictors our estimates can be noisy.
 
 + We wish to perform regularization to give more stable estimates and perform shrinkage
 
 + We will consider the Bayesian Lasso and a Rdige Regression in an attempt to produce better estimates.
 
 + Recall the Bayesian Lasso uses the double exponential prior and the Bayesian Ridge regression utilizes a normal prior.

---

# L1 LASSO

&lt;br&gt;

&lt;br&gt;

Likelihood



`$$\begin{align*} \mathcal{L}(y|z,\gamma,\beta,X) &amp; \propto \prod_{i=1}^{n} \left[ \ \sum_{j=1}^5 I_{ \{ \gamma_{j-1} &lt; z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z_i - x'_i \beta \right)^2 \right] \\[.5em] &amp; \propto  \left[ \ \prod_{i=1}^{n} \sum_{j=1}^5 I_{ \{ \gamma_{j-1} &lt; z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z-X\beta\right)'\left(z-X\beta\right) \right] \\[1em] \end{align*}$$`


&lt;br&gt;

---

# L1 LASSO

Posterior

 + Instead of applying the Laplacian prior to directly to the likelihood, we can express the Laplacian as a mixture model of normals with inverse gamma priors. 

--

 + Previously: `\(\beta_k \sim L(0 , \lambda^{-1})\)`
 
 
--
 
 + Now: `\(\beta_k \sim N\left(0 , \cfrac{4 \left(\lambda^{-1}\right)^2 }{\alpha_k} \right)\)`, where `\(\left(\lambda^{-1}\right)^2 \sim IG(a,b)\)` and `\(\alpha_k \overset{iid}{\sim} IG\left(1,\frac{1}{2} \right)\)`

--

 + This allows us to utilize a Gibbs sampler

---

# L1 LASSO

&lt;br&gt;

Full Posterior

`$$\begin{align*} P(z,\gamma,\beta,\alpha,\lambda|X,y) &amp; \propto \mathcal{L}(y|z,\gamma,\beta,X) \pi(\beta|\lambda,\alpha)\pi(\lambda)\pi(\alpha)    \end{align*}$$` 
&lt;br&gt;

`$$\begin{align*} &amp; \propto  \left[ \ \prod_{i=1}^{n} \sum_{j=1}^5 I_{ \{ \gamma_{j-1} &lt; z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z-X\beta\right)'\left(z-X\beta\right) \right] \left[\left(\lambda^{-1}\right)^2\right]^{-(a+1)} \cdot \\[.5em] &amp; \hspace{10 mm} \exp\left(-\cfrac{b}{ \ \left(\lambda^{-1}\right)^2 \ }\right) \prod_{k=1}^K \left( \cfrac{\alpha_k}{\left(\lambda^{-1}\right)^2} \right)^\frac{1}{2} \exp\left[-\cfrac{\alpha_k \ \beta_k^2}{8\left(\lambda^{-1}\right)^2} \right] \alpha_k^{-2} \exp\left(-\cfrac{1}{2\alpha_k}\right) \end{align*}$$`


---

# L1 LASSO

&lt;br&gt;

The full conditionals of `\(z\)` and `\(\gamma_j\)` are not affected by the new prior.

--

Recall : 

--

If `\(y_i = j\)`, the conditional for `\(z_i\)` is:


`$$\begin{align*} p( z| \gamma,\beta,\alpha,\lambda,X,y ) &amp; \propto  \exp\left[-\cfrac{1}{2}\left(z_i - x'_i\beta \right)^2\right]\end{align*}  I_{ \{ \gamma_{j-1} &lt; z_i \leq \gamma_{j} \} }$$`

A truncated normal with mean `\(x'_i\beta\)`, variance 1, and truncated to be between `\(\gamma_{j-1}\)` and `\(\gamma_j\)`.


--


The conditional for `\(\gamma_j\)` is:

`$$\gamma_j|z,\beta,X,y \sim U\Big(\max\{\max\{z_i: y_i = j\}, \gamma_{j-1} \}, \min\{\min\{ z_i:y_i = j+1, \gamma_{j+1}\}\}\Big)$$`
---

# L1 LASSO

&lt;br&gt;

The conditional for `\(\beta\)` is:

`$$\scriptsize \begin{align*} P(\beta|\text{rest}) &amp; \propto \exp\left\{-\cfrac{1}{2} \left[ \left( \beta  - \left( X'X + \frac{\lambda^2}{4}\bf{D}_\alpha \right) X'Z   \right)'\left( X'X + \frac{\lambda^2}{4}\bf{D}_\alpha \right) \left( \beta  - \left( X'X + \frac{\lambda^2}{4}\bf{D}_\alpha \right) X'Z   \right) \right]\right\}\end{align*}$$` 

Where `\(\bf{D}_\alpha\)` `\(= \text{diag}(\alpha_1, ...,\alpha_k)\)`

--



&lt;br&gt;

We recognize this as the kernel of a normal distribution, so

`$$\small  \beta|\text{rest} \sim N\left( \left( X'X + \frac{\lambda^2}{4}\bf{D}_\alpha \right) X'Z  , \left( X'X + \frac{\lambda^2}{4}\bf{D}_\alpha \right)^{-1} \right)$$`

---

# L1 LASSO

&lt;br&gt;

The conditional on `\(\lambda\)` is:

$$P(\lambda|z, \gamma,\beta,\alpha,X,y) \propto \left(\lambda^2\right)^{\frac{K}{2}+a+1} \exp\left[-\lambda^2 \left(b+ \frac{1}{8} \sum_{k=1}^K \alpha_k\beta_k^2 \right)\right] $$

Which is identifiable as the kernel of the a Gamma.

`$$\lambda|z, \gamma,\beta,\alpha,X,y \sim Gamma\left( \frac{K}{2} + a + 2, b+ \frac{1}{8} \sum_{k=1}^K \alpha_k\beta_k^2 \right)$$`

---


# L1 LASSO

&lt;br&gt;

Finally, the conditional for `\(\alpha_p\)`


`$$P(\alpha_p |\lambda,z, \gamma,\beta,\alpha_{p\neq k},X,y) \propto \alpha_p^{-\frac{3}{2}} \exp\left[ -\cfrac{1}{2} \cfrac{\left( \alpha_p - \frac{2\lambda^{-1}}{|\beta_p|} \right)^2}{\alpha_p\left(\frac{2\lambda^{-1}}{|\beta_p|}\right)^2} \right]$$`


Which is a kernel of the inverse Gaussian.

`$$\alpha_p |\lambda,z, \gamma,\beta,\alpha_{p\neq k},X,y \sim N^{-1}\left(  \frac{ 2  \lambda^{-1}  }{|\beta_k|} , 1 \right)$$`

---

# L1 LASSO

After deriving the conditionals we can set up a Gibbs sampler without a M-H step.

--

1. For each `\(j = 2,...,J\)` draw `\(\small\gamma_j^{(b)}\)` from `$$\scriptsize U\Big(\max\left\{\max\left\{z_i^{(b-1)}: y_i = j\right\}, \gamma_{j-1}^{(b-1)} \right\}, \min\left\{\min\left\{ z_i^{(b-1)}:y_i = j+1 \right\}, \gamma_{j+1}^{(b-1)} \right\} \Big)$$`

--

1. For each `\(j = 1,...,J\)` draw `\(z_i^{(b)} | y_i = j\)` from


`$$\small N\left( x'_i \beta^{(b-1)} , 1 \right) \text{, truncated at the left (right) by } \gamma_{j-1}^{(b-1)} \left( \gamma_{j}^{(b-1)}\right)$$`
--

3. Draw `\(\lambda^{2^{(b)}}\)` from `\(Gamma\left( \frac{K}{2} + a + 2, b+ \frac{1}{8} \sum_{k=1}^K \alpha_k^{(b-1)} \beta_k^{2^{(b-1)}} \right)\)`

--

4. For each `\(k = 1,...,K\)` draw `\(\alpha_k^{(b)}\)` from `\(N^{-1}\left( \frac{ 2  \lambda^{-1^{(b)}}  }{|\beta_k^{(b-1)}|} , 1\right)\)`

--

5. Draw `\(\beta^{(b)}\)` from 

`$$\small N\left( \left( X'X + \frac{\lambda^{2^{(b)}}}{4}\bf{D}^{(b)}_\alpha \right) X'Z^{(b)}  , \left( X'X + \frac{\lambda^{2^{(b)}}}{4}\bf{D}^{(b)}_\alpha \right)^{-1} \right)$$`
---


# L2 Ridge regression

&lt;br&gt;

Lastly, we consider the Bayesian Ridge regression, which is `\(\beta_k \sim N\left(0,\lambda^{-1}\right)\)`. Thus we have:
 
`$$\begin{align*}\pi\left(\beta_1,...,\beta_K\right) \propto \prod_{k=1}^K \lambda^{\frac{1}{2}} \ \exp\left[-\cfrac{\lambda}{2} \beta_k^2\right] \\[.5em] \propto \lambda^\frac{K}{2} \exp\left[-\cfrac{\lambda}{2} \sum_{k=1}^K \beta_k^2 \right] \end{align*}$$`

--

With Jeffrey's prior as the hyper-prior:

`$$\pi(\lambda) \propto \lambda^{-1}$$`
---

This leads to the posterior:

&lt;br&gt;

`$$\small \begin{align*}P(\beta,\lambda,z|y) &amp; \propto \mathcal{L}(y|\beta,\lambda,z)\pi(\beta|\lambda)\pi(\lambda)\\[.5em] &amp; \propto  \left[ \ \prod_{i=1}^{n} \sum_{j=1}^5 I_{ \{ \gamma_{j-1} &lt; z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z-X\beta\right)'\left(z-X\beta\right) \right]  \cdot  \\[.5em] &amp; \hspace{10mm} \exp\left[-\cfrac{1}{2} (z-X\beta)'(z-X\beta) \right]  \lambda^\frac{K}{2} \exp\left[-\cfrac{\lambda}{2} \sum_{k=1}^K \beta_k^2 \right]\lambda^{-1}\\[.5em] &amp; \propto \left[ \ \prod_{i=1}^{n} \sum_{j=1}^5 I_{ \{ \gamma_{j-1} &lt; z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \lambda^{\frac{K}{2}-1} \exp\left[-\cfrac{1}{2} \Big( (z-X\beta)'(z-X\beta) - \lambda B'B \Big)\right]\end{align*}$$`

---


Thus the full conditional on `\(\lambda\)` is:

`$$\small P(\lambda|\text{rest}) \propto \lambda^{\frac{K}{2}-1} \exp\left[-\cfrac{1}{2} \  B'B\right]$$`

--

Which is recognizable as a `\(\small Gamma\left(\cfrac{K}{2}, \cfrac{1}{2} B'B \right)\)`

--

&lt;br&gt;

The conditional of `\(\beta\)` is:

`$$\small P(\beta|\text{rest}) \propto \exp\left[-\cfrac{1}{2}\left( B'(X'X+\lambda I_K)\beta - 2\beta(X'X+\lambda I_K)(X'X+\lambda I_K)^{-1}X'z\right) \right]$$`
--

Which is the kernel of a multivariate normal random variable with distribution `$$\beta|\text{rest} \sim N\left((X'X+\lambda I_K)^{-1}X'z, (X'X+\lambda I_K)^{-1} \right)$$`

---
&lt;br&gt;

The full conditionals of `\(z\)` and `\(\gamma_j\)` are not affected by the new prior.

--

Recall : 



If `\(y_i = j\)`, the conditional for `\(z_i\)` is:


`$$\begin{align*} p( z| \text{rest} ) &amp; \propto  \exp\left[-\cfrac{1}{2}\left(z_i - x'_i\beta \right)^2\right]\end{align*}  I_{ \{ \gamma_{j-1} &lt; z_i \leq \gamma_{j} \} }$$`

A truncated normal with mean `\(x'_i\beta\)`, variance 1, and truncated to be between `\(\gamma_{j-1}\)` and `\(\gamma_j\)`.


--


The conditional for `\(\gamma_j\)` is:

`$$\gamma_j|\text{rest} \sim U\Big(\max\{\max\{z_i: y_i = j\}, \gamma_{j-1} \}, \min\{\min\{ z_i:y_i = j+1, \gamma_{j+1}\}\}\Big)$$`
---

We finally specifiy a Gibbs sampling scheme
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
