---
title: "Making News Go Viral"
subtitle: "Math 640 Class Project Presentation"
author: "Cin Cin Fang & Michael Leibert"
date: "(updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["metropolis", "metropolis-fonts", "test.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "macros.js"
header-includes:
   - \usepackage[english]{babel}
   - \usepackage[autostyle, english = american]{csquotes}
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
# find.image searches subdirectories for partial match of argument. Useful when some images are dynamically generated
find.image=function(name){grep(name,list.files(recursive = TRUE),value=TRUE)}

# Example of how to use find.image to display an image
#![:scaleimgpct 100%](`r find.image("mnist_digits")`)

```

#Data


Online News Popularity Data Set 

 + This dataset contains rougly 45 attributes about articles published by Mashable in a period of two years. The goal is to predict the number of shares in social networks (popularity).
 
 + Variables range from category (business, entertainment, lifestyle, etc), measures of the article's subjectivity and polarity, and the average length words.
 
 + We end up removing some variables that display multicollinearity, and we also scale the continuous variables.

 + Dataset has 39,644 samples. We will use a 30,000 train / 9,644 test split.
---

#Data


A sample of a few rows and columns:

```{r, echo = F}
onp <- read.csv("onpex.csv",stringsAsFactors = F)
onp <- rbind( onp , rep(" ", 7) )
print(onp[ ,c(3,12,13,29, 40 , 47 , 45)], row.names = F)
```



---
# Goals


<br>

<br>

<br>


 + The goal is to find the attributes that can predict how sharable an article is.


--


 + We will utilize the Bayesian ordinal probit model.


--

 + We will test the robustness of our estimates for $\beta$ with three different priors on $\beta$: a flat prior, a Laplacian prior, and a Normal prior.

--

 + We will out-of-sample testing to check our results.


---
# Methods

 + Rather than attempt to predict the given "shares" directly, we instead translate "shares" of the articles $i = 1,...,n$ into quintiles to form ordinal categories.

--

 + Again, this lends itself to ordinal probit regression, which can be more easily modeled in a Bayesian context through latent variables.



--

 + For this formulation, the latent variable $z_i$ of article $i$, $z \overset{iid}{\sim} N( x'_i \beta ,1)$

 + Where $x_i$ is the vector of predictive variables and $\beta$ is the vector of coeffcients we are ultimately interested in.
 
--

The observed response variable $y_i$, which quantile of shares does it attract, is:


$$y_i = \begin{cases} \hfil 1 &  \text{if  } \  \gamma_0 < z_i \leq \gamma_1 \\[.3 em] \hfil 2  & \text{if  } \ \gamma_1 < z_i \leq \gamma_2 \\[.3 em] & \hspace{11 mm} \vdots \\[.4 em] \hfil 5  & \text{if  } \ \gamma_4 < z_i \leq \gamma_5 \\ \end{cases}$$


Where $\gamma_0$ is $-\infty$, $\gamma_1$ is fixed at 0, and $\gamma_5$ is $\infty$

---

# Flat Prior

<br>

Likelihood



$$\begin{align*} \mathcal{L}(y|z,\gamma,\beta,X) & \propto \prod_{i=1}^{n} \left[ \ \sum_{j=1}^5 I_{ \{ \gamma_{j-1} < z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z_i - x'_i \beta \right)^2 \right] \\[.5em] & \propto  \left[ \ \prod_{i=1}^{n} \sum_{j=1}^5 I_{ \{ \gamma_{j-1} < z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z-X\beta\right)'\left(z-X\beta\right) \right] \\[1em] \end{align*}$$


<br>

--

Under the flat prior $\pi(\beta)\propto 1$, the posterior is unchanged.  

<br>

$$\begin{align*} P(z,\gamma,\beta|y,X) &  \propto  \left[ \ \prod_{i=1}^{n} \sum_{j=1}^5 I_{ \{ \gamma_{j-1} < z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z-X\beta\right)'\left(z-X\beta\right) \right] \end{align*}$$ 


---

# Flat Prior

We now derive the conditional for $\beta$:

$$\begin{align*} p(\beta|z,\gamma,X,y) & \propto \exp\left[-\cfrac{1}{2} \left( \beta'X'X\beta - 2\beta'X'X(X'X)^{-1}X'z \right) \right]\end{align*}$$

--

Which is the kernel of a normal distribution, so $$\begin{align*} \beta|z,\gamma,X,y \sim N\Big( (X'X)^{-1}X'z , (X'X)^{-1}  \Big)\end{align*}$$

<br>
--

The conditional for $\gamma_j$, where $j=2,3,4$:

$$\begin{align*} p(\gamma_j|z,\beta,X,y) & \propto \prod_{i=1}^n I_{ \{ \gamma_{j-1} < z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}} + I_{ \{ \gamma_{j} < z_i \leq \gamma_{j+1} \} }  \ I_{\{y_i = j + 1 \}}\end{align*}$$
--

Which is a uniform distribution

$$\gamma_j|z,\beta,X,y \sim U\Big(\max\{\max\{z_i: y_i = j\}, \gamma_{j-1} \}, \min\{\min\{ z_i:y_i = j+1\}, \gamma_{j+1}\} \Big)$$


---

#Flat Prior


<br>

<br>

To find the conditionals for $z_i$, we must consider the separate cases of $y_i$

--

If $y_i = j$, the conditional for $z_i$ is:


$$\begin{align*} p( z |\gamma,\beta,X,y) & \propto  \exp\left[-\cfrac{1}{2}\left(z_i - x'_i\beta \right)^2\right]\end{align*}  I_{ \{ \gamma_{j-1} < z_i \leq \gamma_{j} \} }$$


Which is a truncated normal with mean $x'_i\beta$, variance 1, and truncated to be between $\gamma_{j-1}$ and $\gamma_j$.




---

# Penalization

<br>

 + With a large number of predictors our estimates can be noisy.
 
 + We wish to perform regularization to give more stable estimates and perform shrinkage
 
 + We will consider the Bayesian Lasso and a Rdige Regression in an attempt to produce better estimates.
 
 + Recall the Bayesian Lasso uses the double exponential prior and the Bayesian Ridge regression utilizes a normal prior.

---

# L1 LASSO

<br>

<br>

Likelihood



$$\begin{align*} \mathcal{L}(y|z,\gamma,\beta,X) & \propto \prod_{i=1}^{n} \left[ \ \sum_{j=1}^5 I_{ \{ \gamma_{j-1} < z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z_i - x'_i \beta \right)^2 \right] \\[.5em] & \propto  \left[ \ \prod_{i=1}^{n} \sum_{j=1}^5 I_{ \{ \gamma_{j-1} < z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z-X\beta\right)'\left(z-X\beta\right) \right] \\[1em] \end{align*}$$


<br>

---

# L1 LASSO

Posterior

 + Instead of applying the Laplacian prior to directly to the likelihood, we can express the Laplacian as a mixture model of normals with inverse gamma priors. 

--

 + Previously: $\beta_k \sim L(0 , \lambda^{-1})$
 
 
--
 
 + Now: $\beta_k \sim N\left(0 , \cfrac{4 \left(\lambda^{-1}\right)^2 }{\alpha_k} \right)$, where $\left(\lambda^{-1}\right)^2 \sim IG(a,b)$ and $\alpha_k \overset{iid}{\sim} IG\left(1,\frac{1}{2} \right)$

--

 + This allows us to utilize a Gibbs sampler

---

# L1 LASSO

<br>

Full Posterior

$$\begin{align*} P(z,\gamma,\beta,\alpha,\lambda|X,y) & \propto \mathcal{L}(y|z,\gamma,\beta,X) \pi(\beta|\lambda,\alpha)\pi(\lambda)\pi(\alpha)    \end{align*}$$ 
<br>

$$\begin{align*} & \propto  \left[ \ \prod_{i=1}^{n} \sum_{j=1}^5 I_{ \{ \gamma_{j-1} < z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z-X\beta\right)'\left(z-X\beta\right) \right] \left[\left(\lambda^{-1}\right)^2\right]^{-(a+1)} \cdot \\[.5em] & \hspace{10 mm} \exp\left(-\cfrac{b}{ \ \left(\lambda^{-1}\right)^2 \ }\right) \prod_{k=1}^K \left( \cfrac{\alpha_k}{\left(\lambda^{-1}\right)^2} \right)^\frac{1}{2} \exp\left[-\cfrac{\alpha_k \ \beta_k^2}{8\left(\lambda^{-1}\right)^2} \right] \alpha_k^{-2} \exp\left(-\cfrac{1}{2\alpha_k}\right) \end{align*}$$


---

# L1 LASSO

<br>

The full conditionals of $z$ and $\gamma_j$ are not affected by the new prior.

--

Recall : 

--

If $y_i = j$, the conditional for $z_i$ is:


$$\begin{align*} p( z| \gamma,\beta,\alpha,\lambda,X,y ) & \propto  \exp\left[-\cfrac{1}{2}\left(z_i - x'_i\beta \right)^2\right]\end{align*}  I_{ \{ \gamma_{j-1} < z_i \leq \gamma_{j} \} }$$

A truncated normal with mean $x'_i\beta$, variance 1, and truncated to be between $\gamma_{j-1}$ and $\gamma_j$.


--


The conditional for $\gamma_j$ is:

$$\gamma_j|z,\beta,X,y \sim U\Big(\max\{\max\{z_i: y_i = j\}, \gamma_{j-1} \}, \min\{\min\{ z_i:y_i = j+1, \gamma_{j+1}\}\}\Big)$$
---

# L1 LASSO

<br>

The conditional for $\beta$ is:

$$\scriptsize \begin{align*} P(\beta|\text{rest}) & \propto \exp\left\{-\cfrac{1}{2} \left[ \left( \beta  - \left( X'X + \frac{\lambda^2}{4}\bf{D}_\alpha \right) X'Z   \right)'\left( X'X + \frac{\lambda^2}{4}\bf{D}_\alpha \right) \left( \beta  - \left( X'X + \frac{\lambda^2}{4}\bf{D}_\alpha \right) X'Z   \right) \right]\right\}\end{align*}$$ 

Where $\bf{D}_\alpha$ $= \text{diag}(\alpha_1, ...,\alpha_k)$

--



<br>

We recognize this as the kernel of a normal distribution, so

$$\small  \beta|\text{rest} \sim N\left( \left( X'X + \frac{\lambda^2}{4}\bf{D}_\alpha \right) X'Z  , \left( X'X + \frac{\lambda^2}{4}\bf{D}_\alpha \right)^{-1} \right)$$

---

# L1 LASSO

<br>

The conditional on $\lambda$ is:

$$P(\lambda|z, \gamma,\beta,\alpha,X,y) \propto \left(\lambda^2\right)^{\frac{K}{2}+a+1} \exp\left[-\lambda^2 \left(b+ \frac{1}{8} \sum_{k=1}^K \alpha_k\beta_k^2 \right)\right] $$

Which is identifiable as the kernel of the a Gamma.

$$\lambda|z, \gamma,\beta,\alpha,X,y \sim Gamma\left( \frac{K}{2} + a + 2, b+ \frac{1}{8} \sum_{k=1}^K \alpha_k\beta_k^2 \right)$$

---


# L1 LASSO

<br>

Finally, the conditional for $\alpha_p$


$$P(\alpha_p |\lambda,z, \gamma,\beta,\alpha_{p\neq k},X,y) \propto \alpha_p^{-\frac{3}{2}} \exp\left[ -\cfrac{1}{2} \cfrac{\left( \alpha_p - \frac{2\lambda^{-1}}{|\beta_p|} \right)^2}{\alpha_p\left(\frac{2\lambda^{-1}}{|\beta_p|}\right)^2} \right]$$


Which is a kernel of the inverse Gaussian.

$$\alpha_p |\lambda,z, \gamma,\beta,\alpha_{p\neq k},X,y \sim N^{-1}\left(  \frac{ 2  \lambda^{-1}  }{|\beta_k|} , 1 \right)$$

---

# L1 LASSO

After deriving the conditionals we can set up a Gibbs sampler without a M-H step.

--

1. For each $j = 2,...,J$ draw $\small\gamma_j^{(b)}$ from $$\scriptsize U\Big(\max\left\{\max\left\{z_i^{(b-1)}: y_i = j\right\}, \gamma_{j-1}^{(b-1)} \right\}, \min\left\{\min\left\{ z_i^{(b-1)}:y_i = j+1 \right\}, \gamma_{j+1}^{(b-1)} \right\} \Big)$$

--

1. For each $j = 1,...,J$ draw $z_i^{(b)} | y_i = j$ from


$$\small N\left( x'_i \beta^{(b-1)} , 1 \right) \text{, truncated at the left (right) by } \gamma_{j-1}^{(b-1)} \left( \gamma_{j}^{(b-1)}\right)$$
--

3. Draw $\lambda^{2^{(b)}}$ from $Gamma\left( \frac{K}{2} + a + 2, b+ \frac{1}{8} \sum_{k=1}^K \alpha_k^{(b-1)} \beta_k^{2^{(b-1)}} \right)$

--

4. For each $k = 1,...,K$ draw $\alpha_k^{(b)}$ from $N^{-1}\left( \frac{ 2  \lambda^{-1^{(b)}}  }{|\beta_k^{(b-1)}|} , 1\right)$

--

5. Draw $\beta^{(b)}$ from 

$$\small N\left( \left( X'X + \frac{\lambda^{2^{(b)}}}{4}\bf{D}^{(b)}_\alpha \right) X'Z^{(b)}  , \left( X'X + \frac{\lambda^{2^{(b)}}}{4}\bf{D}^{(b)}_\alpha \right)^{-1} \right)$$
---


# L2 Ridge regression

<br>

Lastly, we consider the Bayesian Ridge regression, which is $\beta_k \sim N\left(0,\lambda^{-1}\right)$. Thus we have:
 
$$\begin{align*}\pi\left(\beta_1,...,\beta_K\right) \propto \prod_{k=1}^K \lambda^{\frac{1}{2}} \ \exp\left[-\cfrac{\lambda}{2} \beta_k^2\right] \\[.5em] \propto \lambda^\frac{K}{2} \exp\left[-\cfrac{\lambda}{2} \sum_{k=1}^K \beta_k^2 \right] \end{align*}$$

--

With Jeffrey's prior as the hyper-prior:

$$\pi(\lambda) \propto \lambda^{-1}$$
---

This leads to the posterior:

<br>

$$\small \begin{align*}P(\beta,\lambda,z|y) & \propto \mathcal{L}(y|\beta,\lambda,z)\pi(\beta|\lambda)\pi(\lambda)\\[.5em] & \propto  \left[ \ \prod_{i=1}^{n} \sum_{j=1}^5 I_{ \{ \gamma_{j-1} < z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \exp\left[ -\cfrac{1}{2} \left(z-X\beta\right)'\left(z-X\beta\right) \right]  \cdot  \\[.5em] & \hspace{10mm} \exp\left[-\cfrac{1}{2} (z-X\beta)'(z-X\beta) \right]  \lambda^\frac{K}{2} \exp\left[-\cfrac{\lambda}{2} \sum_{k=1}^K \beta_k^2 \right]\lambda^{-1}\\[.5em] & \propto \left[ \ \prod_{i=1}^{n} \sum_{j=1}^5 I_{ \{ \gamma_{j-1} < z_i \leq \gamma_j \} }  \ I_{\{y_i = j\}}\right] \lambda^{\frac{K}{2}-1} \exp\left[-\cfrac{1}{2} \Big( (z-X\beta)'(z-X\beta) - \lambda B'B \Big)\right]\end{align*}$$

---


Thus the full conditional on $\lambda$ is:

$$\small P(\lambda|\text{rest}) \propto \lambda^{\frac{K}{2}-1} \exp\left[-\cfrac{1}{2} \  B'B\right]$$

--

Which is recognizable as a $\small Gamma\left(\cfrac{K}{2}, \cfrac{1}{2} B'B \right)$

--

<br>

The conditional of $\beta$ is:

$$\small P(\beta|\text{rest}) \propto \exp\left[-\cfrac{1}{2}\left( B'(X'X+\lambda I_K)\beta - 2\beta(X'X+\lambda I_K)(X'X+\lambda I_K)^{-1}X'z\right) \right]$$
--

Which is the kernel of a multivariate normal random variable with distribution $$\beta|\text{rest} \sim N\left((X'X+\lambda I_K)^{-1}X'z, (X'X+\lambda I_K)^{-1} \right)$$

---
<br>

The full conditionals of $z$ and $\gamma_j$ are not affected by the new prior.

--

Recall : 



If $y_i = j$, the conditional for $z_i$ is:


$$\begin{align*} p( z| \text{rest} ) & \propto  \exp\left[-\cfrac{1}{2}\left(z_i - x'_i\beta \right)^2\right]\end{align*}  I_{ \{ \gamma_{j-1} < z_i \leq \gamma_{j} \} }$$

A truncated normal with mean $x'_i\beta$, variance 1, and truncated to be between $\gamma_{j-1}$ and $\gamma_j$.


--


The conditional for $\gamma_j$ is:

$$\gamma_j|\text{rest} \sim U\Big(\max\{\max\{z_i: y_i = j\}, \gamma_{j-1} \}, \min\{\min\{ z_i:y_i = j+1, \gamma_{j+1}\}\}\Big)$$
---

We finally specifiy a Gibbs sampling scheme
