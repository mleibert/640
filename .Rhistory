spirals <- spiralpred <- mlbench.spirals(75,1.5,.07)
y <- as.numeric(spirals$classes) - 1
x <- t(spirals$x )
nnet1.fit.batch( x  , y , 1 , 3 ,  100 , 1500, 3,	Activation = sigmoid, Output = Sigmoid, 32 )
nnet1.fit.batch
nnet1.fit.batch
library(keras)
install.packages("Matrix")
library(keras)
nnet1.fit.batch
install_keras(tensorflow = "gpu",method = "conda")
install.packages('MCMCpack')
amazon
x_train
rm(list=ls())
library(keras)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
amdir <- "C:\\Users\\Administrator\\Documents\\reviews.csv"
amazon <- read.csv(  amdir , stringsAsFactors= F )
amazon$Text <- as.character(amazon$Text )
one.hot <- function(Z){return(unname( as.matrix(
as.data.frame( t( model.matrix(~ as.factor(Z) + 0) ) ) ) )) }
testsamples <- sample( 1:nrow(amazon)   , ((nrow(amazon) )/ 2 ))
trainsamples <-  setdiff(  1:nrow(amazon)   , testsamples)
ytrain <- amazon[trainsamples,]$Score
ytest <- amazon[testsamples ,]$Score
length(testsamples ) == length(trainsamples )
samples <- amazon[,10]; rm(amazon)
max_features <- 500
gc()
tokenizer <- text_tokenizer( num_words = max_features   ) %>%
fit_text_tokenizer(samples)
oh_results <- texts_to_matrix(tokenizer,samples, mode = "tfidf")
dim(oh_results)
x_train <- oh_results[trainsamples,]
x_test <- oh_results[testsamples ,]
rm(oh_results);gc()
ytrain <- t( one.hot(ytrain ) )
ytest <- t( one.hot(ytest) )
batch_size <- 32
embedding_dims <- 50
filters <- 250
kernel_size <- 3
hidden_dims <- 250
epochs <- 5
maxlen <- dim(x_train)[2]
model <- keras_model_sequential() %>%
#layer_embedding(input_dim=max_features,
#	 output_dim=embedding_dims, input_length = maxlen) %>%
#layer_dropout(rate=0.2) %>%
#layer_flatten(.) %>%
layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
layer_dropout(0.2) %>% regularizer_l1_l2(l1 = 0.01, l2 = 0.01) %>%
layer_dense(400 , activation = "relu"  )  %>%
layer_dropout(0.2) %>% regularizer_l1_l2(l1 = 0.01, l2 = 0.01) %>%
layer_dense(5 , activation = "softmax" )
model <- keras_model_sequential() %>%
layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
layer_dropout(0.2) %>%
layer_dense(400 , activation = "relu" )  %>%
layer_dropout(0.2) %>%
layer_dense(5 , activation = "softmax" )
gc()
# Compile model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = "adagrad",
metrics = "accuracy"
)
# Training ----------------------------------------------------------------
model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = 20 ,
validation_data = list(x_test, (ytest ) )
)
## RNN's and embedding? What is pad sequences doing
# model %>% save_model_weights_hdf5("adagrad.h5")
model <- keras_model_sequential() %>%
layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
layer_dropout(0.2) %>%
layer_dense(400 , activation = "relu" )  %>%
layer_dropout(0.2) %>%
layer_dense(400 , activation = "relu" )  %>%
layer_dropout(0.2) %>%
layer_dense(5 , activation = "softmax" )
# Compile model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = "adagrad",
metrics = "accuracy"
)
# Training ----------------------------------------------------------------
model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = 20 ,
validation_data = list(x_test, (ytest ) )
)
model
model %>% save_model_weights_hdf5("asdagrad.h5")
rm(list=ls())
gc()
library(keras)
require("readr")
library(stringr)
library(purrr)
library(tokenizers)
maxlen <- 40
# Data Preparation --------------------------------------------------------
# Retrieve text
path <- get_file(
'nietzsche.txt',
origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt'
)
# Load, collapse, and tokenize text
text <- read_lines(path) %>%
str_to_lower() %>%
str_c(collapse = "\n") %>%
tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)
print(sprintf("corpus length: %d", length(text)))
chars <- text %>%
unique() %>%
sort()
print(sprintf("total chars: %d", length(chars)))
# Cut the text in semi-redundant sequences of maxlen characters
dataset <- map(
seq(1, length(text) - maxlen - 1, by = 3),
~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
)
dataset <- transpose(dataset)
# Vectorization
x <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))
y <- array(0, dim = c(length(dataset$sentece), length(chars)))
for(i in 1:length(dataset$sentece)){
x[i,,] <- sapply(chars, function(x){
as.integer(x == dataset$sentece[[i]])
})
y[i,] <- as.integer(chars == dataset$next_char[[i]])
}
# Model Definition --------------------------------------------------------
model <- keras_model_sequential()
model %>%
layer_lstm(128, input_shape = c(maxlen, length(chars))) %>%
layer_dense(length(chars)) %>%
layer_activation("softmax")
optimizer <- optimizer_rmsprop(lr = 0.01)
model %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer
)
head(x)
dim(x)
nrow(path)
path
nrow(text)
x
library(beepr)
library(keras)
librarY(tm)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
amazon <- read.csv("reviews.csv", stringsAsFactors= F)
one.hot <- function(Z){return(unname( as.matrix(
as.data.frame( t( model.matrix(~ as.factor(Z) + 0) ) ) ) )) }
testsamples <- sample( 1:nrow(amazon)   , ((nrow(amazon) )/ 2 ))
trainsamples <-  setdiff(  1:nrow(amazon)   , testsamples)
ytrain <- amazon[trainsamples,]$Score
ytest <- amazon[testsamples ,]$Score
length(testsamples ) == length(trainsamples )
samples <- amazon[,10]; rm(amazon)
samples <- removeWords(samples, stopwords("en"))
samples <- stripWhitespace(samples)
max_features <- 100
gc()
tokenizer <- text_tokenizer( num_words = max_features   ) %>%
fit_text_tokenizer(samples)
oh_results <- texts_to_matrix(tokenizer,samples, mode = "tfidf")
dim(oh_results)
x_train <- oh_results[trainsamples,]
x_test <- oh_results[testsamples ,]
rm(oh_results)
ytrain <- t( one.hot(ytrain ) )
ytest <- t( one.hot(ytest) )
batch_size <- 32
embedding_dims <- 50
filters <- 250
kernel_size <- 3
hidden_dims <- 250
epochs <- 3
maxlen <- dim(x_train)[2]
#
# model <- keras_model_sequential() %>%
#   #layer_embedding(input_dim=max_features,
# 	#	 output_dim=embedding_dims, input_length = maxlen) %>%
#   #layer_dropout(rate=0.2) %>%
#   #layer_flatten(.) %>%
#   layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(400 , activation = "relu"  )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(5 , activation = "softmax" )
#
L1 = 0
L2 = 0
DO = 0
OP = "adam"
Node = 200
model <- keras_model_sequential() %>%
layer_dense(Node , activation = "relu", input_shape = maxlen,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2) )  %>%
layer_dropout(DO) %>%
layer_dense(Node , activation = "relu", input_shape = maxlen,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2) )  %>%
layer_dropout(DO) %>%
layer_dense(Node , activation = "relu", input_shape = maxlen ,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2))  %>%
layer_dropout(DO) %>%
# layer_dense(Node , activation = "relu", input_shape = maxlen,
#             kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2))  %>%
#             layer_dropout(DO) %>%
layer_dense(5 , activation = "softmax" )
# Compile model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = OP,
metrics = "accuracy"
)
# Training ----------------------------------------------------------------
model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = epochs ,
validation_data = list(x_test, (ytest ) )
)
library(keras)
library(tm)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
amazon <- read.csv("reviews.csv", stringsAsFactors= F)
one.hot <- function(Z){return(unname( as.matrix(
as.data.frame( t( model.matrix(~ as.factor(Z) + 0) ) ) ) )) }
testsamples <- sample( 1:nrow(amazon)   , ((nrow(amazon) )/ 2 ))
trainsamples <-  setdiff(  1:nrow(amazon)   , testsamples)
ytrain <- amazon[trainsamples,]$Score
ytest <- amazon[testsamples ,]$Score
length(testsamples ) == length(trainsamples )
samples <- amazon[,10]; rm(amazon)
samples <- removeWords(samples, stopwords("en"))
samples <- stripWhitespace(samples)
max_features <- 100
gc()
tokenizer <- text_tokenizer( num_words = max_features   ) %>%
fit_text_tokenizer(samples)
oh_results <- texts_to_matrix(tokenizer,samples, mode = "tfidf")
dim(oh_results)
x_train <- oh_results[trainsamples,]
x_test <- oh_results[testsamples ,]
rm(oh_results)
ytrain <- t( one.hot(ytrain ) )
ytest <- t( one.hot(ytest) )
batch_size <- 32
embedding_dims <- 50
filters <- 250
kernel_size <- 3
hidden_dims <- 250
epochs <- 3
maxlen <- dim(x_train)[2]
#
# model <- keras_model_sequential() %>%
#   #layer_embedding(input_dim=max_features,
# 	#	 output_dim=embedding_dims, input_length = maxlen) %>%
#   #layer_dropout(rate=0.2) %>%
#   #layer_flatten(.) %>%
#   layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(400 , activation = "relu"  )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(5 , activation = "softmax" )
#
L1 = 0
L2 = 0
DO = 0
OP = "adam"
Node = 200
model <- keras_model_sequential() %>%
layer_dense(Node , activation = "relu", input_shape = maxlen,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2) )  %>%
layer_dropout(DO) %>%
layer_dense(Node , activation = "relu", input_shape = maxlen,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2) )  %>%
layer_dropout(DO) %>%
layer_dense(Node , activation = "relu", input_shape = maxlen ,
kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2))  %>%
layer_dropout(DO) %>%
# layer_dense(Node , activation = "relu", input_shape = maxlen,
#             kernel_regularizer = regularizer_l1_l2(l1 = L1, l2 = L2))  %>%
#             layer_dropout(DO) %>%
layer_dense(5 , activation = "softmax" )
# Compile model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = OP,
metrics = "accuracy"
)
Node
epochs
model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = epochs ,
validation_data = list(x_test, (ytest ) )
)
Fit <- model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = epochs ,
validation_data = list(x_test, (ytest ) )
)
Fit
names( Fit  )
Fit$metrics
save_model_hdf5(model, "blerg.h5" )
getwd()
save_model_hdf5
Fit
save_model_hdf5(Fit , "blerg.h5" )
save_model_hdf5(PP , "blerg.h5" )
save_model_hdf5(model , "blerg.h5" )
Fit
Str(Fit)
str(Fit)
library(keras)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
amdir <- "G:\\math\\514\\cloud\\reviews.csv"
amazon <- read.csv(  amdir , stringsAsFactors= F )
amazon$Text <- as.character(amazon$Text )
one.hot <- function(Z){return(unname( as.matrix(
as.data.frame( t( model.matrix(~ as.factor(Z) + 0) ) ) ) )) }
testsamples <- sample( 1:nrow(amazon)   , ((nrow(amazon) )/ 2 ))
trainsamples <-  setdiff(  1:nrow(amazon)   , testsamples)
ytrain <- amazon[trainsamples,]$Score
ytest <- amazon[testsamples ,]$Score
length(testsamples ) == length(trainsamples )
samples <- amazon[,10]; rm(amazon)
max_features <- 50
gc()
tokenizer <- text_tokenizer( num_words = max_features   ) %>%
fit_text_tokenizer(samples)
oh_results <- texts_to_matrix(tokenizer,samples, mode = "tfidf")
dim(oh_results)
ytrain <- t( one.hot(ytrain ) )
ytest <- t( one.hot(ytest) )
x_train <- oh_results[trainsamples,]
x_test <- oh_results[testsamples ,]
rm(oh_results);gc()
batch_size <- 32
embedding_dims <- 50
filters <- 250
kernel_size <- 3
hidden_dims <- 250
epochs <- 15
maxlen <- dim(x_train)[2]
lr_schedule <- function(epoch, lr) {
if(epoch <= 2) {
0.4
} else if(epoch > 2 && epoch <= 4){
0.01
} else {
0.001
}}
lr_reducer <- callback_learning_rate_scheduler(lr_schedule)
#
# model <- keras_model_sequential() %>%
# 	layer_gru(300) %>%
# 		layer_dropout(0.2) %>%
# 	layer_gru(300 ) %>%
# 		layer_dropout(0.2) %>%
# 	layer_dense(400 , activation = "relu", input_shape = maxlen )  %>%
# 		layer_dropout(0.2) %>%
#  	layer_dense(400 , activation = "relu" )  %>%
# 		layer_dropout(0.2) %>%
#   layer_dense(400 , activation = "relu" )  %>%
#   layer_dropout(0.2) %>%
#   layer_dense(5 , activation = "softmax" )
gc()
NODES <- 30
model <- keras_model_sequential() %>%
layer_dense(400 , activation = "relu"  )  %>%
layer_dropout(0.2)  %>%
layer_lstm(units = 300) %>%
layer_dropout(0.2) %>%
layer_lstm(units = 300 ) %>%
layer_dropout(0.2) %>%
layer_dense(5 , activation = "softmax" )
model <- keras_model_sequential() %>%
#layer_embedding(input_dim=max_features,
#	 output_dim=embedding_dims, input_length = maxlen) %>%
layer_dense(NODES , activation = "relu", input_shape = maxlen )  %>%
layer_dropout(0.2) %>%
layer_dense(NODES , activation = "relu" )  %>%
layer_dropout(0.2) %>%
layer_dense(NODES , activation = "relu"  )  %>%
layer_dropout(0.2) %>%
layer_dense(5 , activation = "softmax" )
callback_learning_rate_scheduler
lr_reducer <- callback_learning_rate_scheduler(lr_schedule , verbose= 1)
lr_reducer <- callback_learning_rate_scheduler(lr_schedule  )
# Compile model
model %>% compile(
loss = "categorical_crossentropy",
optimizer = "adagrad",
metrics = "accuracy"
)
# Training ----------------------------------------------------------------
model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = 5 ,
callbacks = list(
lr_reducer  ),
validation_data = list(x_test, (ytest ) )
)
256*2
############
rm(list=ls());gc()
require(MCMCpack)
require(mvtnorm)
require(MASS)
library(foreign)
require(beepr)
library(msm)
setwd("G:\\math\\640")
onp <- read.csv("onp_train.csv",header = T)
#   head(onp)
onp <- onp[,c(5:44,47,ncol(onp) )]
fit <- polr( as.factor(share_cat) ~ . , data = onp  , method = "probit" )
X <- model.matrix(fit)
X <- X[,-1]
Y <- as.factor( onp[,ncol(onp)] )
B <- 1000
# MLE Betas
betas <- matrix(NA, B , length(coef(fit)))
betas[1,] <- coef(fit)
# MLE Gammas
gammas <- matrix( NA, B, 4 ); gammas[,1] <- 0
gammas[1, 2:4 ] <- fit$zeta[-1]; gammas[1,  ]
# Z's
Z <- matrix(NA, B , nrow(X))
gam <- data.frame( z = Z[1,], Y )
gam$xtb <- apply( X , 1 , function(Q) t(Q ) %*%  betas[1,] )  #cant vectorize...
head(gam)
i = 1
## Horribly slow
for(k in 1:nrow(gam)){
if( gam[k,]$Y == 1 ){ arg1 = -Inf } else { arg1 =
gammas[i,  as.numeric(gam[k,]$Y) -1 ] }
if( gam[k,]$Y == 5 ){ arg2 = Inf } else { arg2 =
gammas[i  ,   as.numeric(gam[k,]$Y) ] }
gam[k,1] <- rtnorm( 1  , gam[k,3] , 1 ,
lower =  arg1 , upper = arg2  )
}
Z[i,] <- gam$z
#gammas <- cbind( rep(10e-15, nrow(gammas) )  , gammas )
xtxi <- solve(t(X)%*%X, tol = 1e-18)
####
system.time(
for( i in 2:B) {
#sample gammas
gam <- data.frame( z = Z[i-1,], Y )
for( j in 2:4 ){
arg1 <- max( gam[which(gam$Y == j),]$z )
arg2 <- min( gam[which(gam$Y == (j+1)),]$z )
arg3 <- c( max(max(arg1, gammas[i,1]  )) , min(min( arg2 , gammas[i-1,3]  )))
gammas[i,j] <- runif( 1, min(arg3),max(arg3)) # gammas[i,]
}
#sample Z's
gam <- data.frame( z = Z[1,], Y )
gam$xtb <- apply( X , 1 , function(Q) t(Q ) %*%  betas[1,] )  #cant vectorize...
head(gam)
## Horribly slow
for(k in 1:nrow(gam)){
if( gam[k,]$Y == 1 ){ arg1 = -Inf } else { arg1 =
gammas[i,  as.numeric(gam[k,]$Y) -1 ] }
if( gam[k,]$Y == 5 ){ arg2 = Inf } else { arg2 =
gammas[i  ,   as.numeric(gam[k,]$Y) ] }
gam[k,1] <- rtnorm( 1  , gam[k,3] , 1 ,
lower =  arg1 , upper = arg2  )
}
Z[i,] <- gam$z
#sample betas
betas[i,] <- rmvnorm(1,  xtxi %*% (t(X)%*%Z[i,]) , xtxi )
}
); beep("mario")
i
plot(density(gammas[,2])); median(gammas[,2])
