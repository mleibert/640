model %>%
fit(
x_train, (ytrain),
batch_size = batch_size,
epochs = 5 ,
callbacks = list(
lr_reducer  ),
validation_data = list(x_test, (ytest ) )
)
256*2
rm(list=ls());gc()
library(keras)
library(tm)
library(textclean)
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
amazon <- read.csv("reviews.csv", stringsAsFactors= F)
one.hot <- function(Z){return(unname( as.matrix(
as.data.frame( t( model.matrix(~ as.factor(Z) + 0) ) ) ) )) }
# blerg <- amazon[ which(amazon$Score == 5) ,][  sample(1:363122, 250000), ]$Id
# amazon <- ( amazon[ -which(amazon$Id %in% blerg ),  ] )
amazon$Summary <- as.character( amazon$Summary )
library(text2vec)
it_train = itoken(amazon$Text,
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = amazon$Id,
progressbar = T)
)
vocab = create_vocabulary(it_train)
vocab = create_vocabulary(it_train)
vocab
samples <- amazon$Summary
samples <- removeWords(samples, stopwords("en"))
samples <- stripWhitespace(samples)
it_train = itoken(samples,
preprocessor = tolower,
tokenizer = word_tokenizer,
ids = amazon$Id,
progressbar = T)
vocab = create_vocabulary(it_train)
vocab
prune_vocabulary(vocab, term_count_min = 2,
doc_proportion_max = 0.8)
library(stringr)
twitter <- read.csv("twitter.csv",header = F)
beep("coin")
head(twitter)
clean_tweet <- twitter$V6
clean_tweet = gsub("&amp", "", unclean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
rm(clean_tweet)
clean_tweet = gsub("&amp", "", twitter$V6)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet
twitter$V6 <- clean_tweet
head(twitter)
twitter <- twitter[,c(1,6)]
head(twitter)
head(twitter)
library(stringr)
twitter <- read.csv("twitter.csv",header = F)
beep("coin")
head(twitter)
library(beepr); beep("coin")
head(twitter)
twitter <- twitter[,c(1,6)]
head(twitter)
write.csv(twitter, "twitter.csv" , row.names = F)
# write.csv(twitter, "twitter.csv" , row.names = F)
clean_tweet = gsub("&amp", "", twitter[,ncol(twitter)])
head(clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
head(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
head(clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
head(clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
head(clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
head(clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
head(clean_tweet)
clean_tweet = gsub("&amp", "", twitter[,ncol(twitter)])
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
head(clean_tweet)
clean_tweet[1]
gsub( "  ", " " clean_tweet[1] )
gsub( "  ", " ", clean_tweet[1] )
clean_tweet = gsub( "  ", " ", clean_tweet  )
head(clean_tweet)
write.csv(twitter, "twitter.csv" , row.names = F)
table(twitter[,1])
twitters <- twitter[ sample(1:nrow(twitter), 20000),]
head(twitters)
table(twitter[,1])
table(twitters[,1])
twitters[which(twitters[,1] == 4),]
twitters[which(twitters[,1] == 4),1] <- 1
table(twitters[,1])
twitters
write.csv(twitter, "tweet.csv" , row.names = F)
300/60
10*30
library(keras)
twitters
head(twitters)
max_features <- 50
gc()
twitters
max_features <- 50
gc()
tokenizer <- text_tokenizer( num_words = max_features   ) %>%
fit_text_tokenizer(twitters$V6)
tweets <- texts_to_matrix(tokenizer,twitters$V6, mode = "tfidf")
predict_savedmodel(tweets, 'binary.h5')
model <- load_model_hdf5("binary.h5")
model %>% predict(model )
model %>% predict(tweets )
head(twitters)
nrow(twitter)
nrow(twitters)
rownames(twitters) <- NULL
write.csv(twitters, "twitters.csv", row.names= F)
rm(list=ls());gc()
library(keras)
library(tm)
library(textclean)
library(tensorflow)
amazon <- read.csv("reviews.csv", stringsAsFactors= F)
require(keras)
library(tm)
library(textclean)
library(SnowballC)
library(lattice)
max_features = 100
custom_stopwords <- tm::stopwords("english")[-which(tm::stopwords("english")=="not" | tm::stopwords("english")=="should" | tm::stopwords("english")=="against"|tm::stopwords("english")=="below"|tm::stopwords("english")=="above"|tm::stopwords("english")=="again"|tm::stopwords("english")=="few"|tm::stopwords("english")=="most")]
summary.clean <- amazon$Summary %>%
tolower(.) %>%
removePunctuation(.,preserve_intra_word_contractions=TRUE, preserve_intra_word_dashes=TRUE) %>%
removeNumbers(.) %>%
stripWhitespace(.) %>%
replace_contraction(.) %>%
removeWords(., custom_stopwords) %>%
stemDocument(.)
head(summary.clean)
require(beepr);beep("coin")
tokenizer.summary <- text_tokenizer(num_words = max_features) %>%
fit_text_tokenizer(summary.clean)
vocab <- tokenizer.summary$word_counts
beep("coin")
barchart(sort(unlist(vocab), decreasing=TRUE)[20:1], col='lightblue', xlab="Term Frequency", main="Most frequently appearing words")
install.packages
install.packages("servr")
library("servr")
library(servr)
install.packages("later")
install.packages("later")
library("later")
library("server")
remotes::install_github('yihui/xaringan', upgrade = TRUE)
xaringan:::inf_mr()
xaringan:::inf_mr()
xaringan:::inf_mr()
xaringan:::inf_mr()
xaringan:::inf_mr()
xaringan:::inf_mr()
#Clean up twitter data
tweet <- read.csv('C:\\Users\Administrator\\Documents\\twitters.csv', stringsAsFactors = FALSE)
#Clean up twitter data
tweet <- read.csv("C:\\Users\\Administrator\\Documents\\twitters.csv", stringsAsFactors = FALSE)
names(tweet) <- c("positive", "text")
head(tweet)
#Distribution by Sentiment:
#Classify Amazon data into positive vs. negative sentiment
n_positive.amazon <- length(which(amazon$Score==4|amazon$Score==5))
amazon <- read.csv("reviews.csv", stringsAsFactors= F)
#Distribution by Sentiment:
#Classify Amazon data into positive vs. negative sentiment
n_positive.amazon <- length(which(amazon$Score==4|amazon$Score==5))
n_negative.amazon <- length(which(amazon$Score==1|amazon$Score==2))
amazon.dist <- c(positive=n_positive.amazon/(n_positive.amazon + n_negative.amazon),
negative=n_negative.amazon/(n_positive.amazon + n_negative.amazon))
#twitter data
n_positive.tweet <- length(which(tweet$positive==1))
n_negative.tweet <- length(which(tweet$positive==0))
tweet.dist <- c(positive=n_positive.tweet/(n_positive.tweet + n_negative.tweet),
negative=n_negative.tweet/(n_positive.tweet + n_negative.tweet))
sentiment_dist <- data.frame(distrib = c(amazon.dist, tweet.dist), source=c("amazon", "amazon", "twitter", "twitter"))
library(ggplot2)
library(reshape2)
df = melt(data.frame(positive=c(n_positive.amazon/(n_positive.amazon + n_negative.amazon),
n_positive.tweet/(n_positive.tweet + n_negative.tweet)),
negative=c(n_negative.amazon/(n_positive.amazon + n_negative.amazon),
n_negative.tweet/(n_positive.tweet + n_negative.tweet)),
SourceData=c("Amazon", "Twitter")),
variable.name="Sentiment")
ggplot(df, aes(SourceData, value, fill=Sentiment)) + geom_bar(position="dodge", stat="identity") + ylim(0,1) + labs(title="Distribution by Sentiment")
#############################################
##Raw data EDA
#############################################
nrow(amazon)
length(unique(amazon$ProductId))
length(unique(amazon$UserId))
length(unique(amazon$ProfileName))
#Example of rating 1 review
amazon[which(amazon$Score==1)[1],c(7,9,10)]
#Example of rating 2 review
amazon[which(amazon$Score==2)[1],c(7,9,10)]
#Example of rating 3 review
amazon[which(amazon$Score==3)[1],c(7,9,10)]
#Example of rating 4 review
amazon[which(amazon$Score==4)[1],c(7,9,10)]
#Example of rating 5 review
amazon[which(amazon$Score==5)[1],c(7,9,10)]
#Distribution of ratings
library(ggplot2)
ggplot(amazon, aes(x=Score)) + geom_histogram(color="lightblue", fill="lightblue", binwidth = 0.5) + labs(title="Distribution of Ratings", x="Rating")
#Max. number of words in a single full text / summary
library(ngram)
#summary text
wc_distrib.summary <- lapply(as.character(amazon$Summary), function(x) wordcount(x))
summary(unlist(wc_distrib.summary))
#full text
wc_distrib.full <- lapply(as.character(amazon$Text), function(x) wordcount(x))
summary(unlist(wc_distrib.full))
wc_distrib.df <- data.frame(wordcount=c(unlist(wc_distrib.summary),unlist(wc_distrib.full)),
text_type=c(rep('Summary',length(unlist(wc_distrib.summary))), rep('Full',length(unlist(wc_distrib.summary)))))
ggplot(wc_distrib.df,  aes(x=wordcount, col=text_type)) + geom_histogram(fill="white", position="dodge",binwidth=5) + xlim(0,200) + ylim(0,60000) + labs(title="Distribution of Word Count", x="Word Count/Review", y="Number of Reviews")
ggplot(amazon, aes(x=Score)) + geom_histogram(color="lightblue", fill="lightblue", binwidth = 0.5) + labs(title="Distribution of Ratings", x="Rating")
set.seed(34567)
Determine the number of reviews for each category
length(which(amazon$Score==1))
length(which(amazon$Score==2))
length(which(amazon$Score==3))
length(which(amazon$Score==4))
length(which(amazon$Score==5))
rating_5_newsize <- floor(runif(1,length(which(amazon$Score==3)), length(which(amazon$Score==1))))
rating_4_newsize <- floor(runif(1,length(which(amazon$Score==3)), length(which(amazon$Score==1))))
rating_5_sample_index <- sample(which(amazon$Score==5), rating_5_newsize, replace=FALSE)
rating_5_sample <- amazon[rating_5_sample_index,]
rating_4_sample_index <- sample(which(amazon$Score==4), rating_4_newsize, replace=FALSE)
rating_4_sample <- amazon[rating_4_sample_index,]
new_sample_index <- c(which(amazon$Score==1), which(amazon$Score==2),which(amazon$Score==3), rating_4_sample_index, rating_5_sample_index)
amazon.new_sample_index <- sample(new_sample_index,length(new_sample_index), replace=FALSE)
amazon.new_sample <- amazon[amazon.new_sample_index,]
ggplot(amazon.new_sample, aes(x=Score)) + geom_histogram(color="lightblue", fill="lightblue", binwidth = 0.5) + labs(title="Distribution of Ratings (new sample)", x="Rating")
ggplot(amazon, aes(x=Score)) + geom_histogram(color="lightblue", fill="lightblue", binwidth = 0.5) + labs(title="Distribution of Ratings", x="Rating")
set.seed(1)
blerg <- amazon[ which(amazon$Score == 5) ,][  sample(1:363122, 250000), ]$Id
amazon <- ( amazon[ -which(amazon$Id %in% blerg ),  ] )
ggplot(amazon, aes(x=Score)) + geom_histogram(color="lightblue", fill="lightblue", binwidth = 0.5) + labs(title="Distribution of Ratings", x="Rating")
table(amazon$Score)
amazon <- read.csv("reviews.csv", stringsAsFactors= F)
table(amazon$Score)
set.seed(1)
blerg <- amazon[ which(amazon$Score == 5) ,][  sample(1:363122, 300000), ]$Id
amazon <- ( amazon[ -which(amazon$Id %in% blerg ),  ] )
blerg <- amazon[ which(amazon$Score == 4) ,][  sample(1:80655, 30000), ]$Id
amazon <- ( amazon[ -which(amazon$Id %in% blerg ),  ] )
table(amazon$Score)
ggplot(amazon, aes(x=Score)) + geom_histogram(color="lightblue", fill="lightblue", binwidth = 0.5) + labs(title="Distribution of Ratings", x="Rating")
xaringan:::inf_mr()
rm(list=ls());gc()
library(MCMCpack)
library(mvtnorm)
library(MASS)
library(foreign)
library(msm)
library(data.table)
library(MCMCpack)
library(statmod)
# setwd("G:\\math\\640")
onp <- read.csv("onp_train.csv",header = T)
vec.remove <- c('X.1','X','url','timedelta','shares','ID','dt',
"n_non_stop_unique_tokens","n_non_stop_words","kw_avg_min","self_reference_max_shares",
"kw_min_min","self_reference_min_shares","kw_max_avg","rate_negative_words",
"avg_negative_polarity","rate_positive_words","title_subjectivity","max_positive_polarity",
"avg_positive_polarity","average_token_length","kw_max_min","global_rate_positive_words",
"kw_max_max")
onp <- onp[,!(colnames(onp) %in% vec.remove)]
## normalize, for penalty
# I know it's not truly noramlized, but since most vars are on a scale 0 to some high number
# or 0 to 1, this seemed close enough
onp$n_tokens_title <- onp$n_tokens_title/max(onp$n_tokens_title)
onp$n_tokens_content <- onp$n_tokens_content/max(onp$n_tokens_content)
onp$n_unique_tokens <- onp$n_unique_tokens/max(onp$n_unique_tokens)
onp$num_hrefs <- onp$num_hrefs/max(onp$num_hrefs)
onp$num_self_hrefs <- onp$num_self_hrefs/max(onp$num_self_hrefs)
onp$num_imgs <- onp$num_imgs/max(onp$num_imgs)
onp$num_videos <- onp$num_videos/max(onp$num_videos)
onp$num_keywords <- onp$num_keywords/max(onp$num_keywords)
onp$kw_min_max <- onp$kw_min_max/max(onp$kw_min_max)
onp$kw_avg_max <- onp$kw_avg_max/max(onp$kw_avg_max)
onp$kw_min_avg <- onp$kw_min_avg/max(onp$kw_min_avg)
onp$kw_avg_avg <- onp$kw_avg_avg/max(onp$kw_avg_avg)
onp$self_reference_avg_sharess <- onp$self_reference_avg_sharess/max(onp$self_reference_avg_sharess)
#### initialize variables
vec.keep <- c('n_tokens_content','num_hrefs','num_self_hrefs','num_imgs','num_videos','num_keywords','kw_min_max',
'kw_avg_max','kw_min_avg','kw_avg_avg','self_reference_avg_sharess','is_weekend','global_subjectivity',
'global_rate_negative_words','min_positive_polarity','title_sentiment_polarity','abs_title_subjectivity',
'abs_title_sentiment_polarity','topic')
sig.formula <- formula(paste0('as.factor(share_cat) ~ ',
paste0(vec.keep,collapse=' +')))
fit <- polr( sig.formula   , data = onp  , method = "probit" )
fit <- polr( as.factor(share_cat) ~ .   , data = onp  , method = "probit" )
X <- model.matrix(fit)
X <- X[,-1]
Y <- as.factor( onp[,'share_cat'] )
K <- ncol(X)
B <- 1000 # aiming for 4000, burn-in and thinning by 10
vec.n <- c(sum(Y==1),
sum(Y==2),
sum(Y==3),
sum(Y==4),
sum(Y==5))
# MLE Betas
dbetas <- function(beta,z,X,lambda){
exp(-0.5*t(z-X%*%beta)%*%(z-X%*%beta)-lambda*sum(abs(beta)))
}
logBetas <- function(beta,z,X,lambda){
-0.5*t(z-X%*%beta)%*%(z-X%*%beta)-lambda*sum(abs(beta))
}
betas <- matrix(NA, B , length(coef(fit)),
dimnames=list(1:B,names(coef(fit))))
beta <- coef(fit)
betas[1,] <- beta
beta.ar <- rep(0,B)
# MLE Gammas
gammas <- matrix( NA, B, 6, dimnames = list(1:B,0:5) )
gammas[,'0'] <- -Inf
gammas[,'1'] <- 0
gammas[,'5'] <- Inf
gammas[1, 3:5 ] <- fit$zeta[-1]
# Z's: initialize with midpoint of range
# now, don't plan to store z's, for memory
# Z <- matrix(NA, B , nrow(X))
z <- rep(NA,nrow(X))
z[Y==1] <- -0.5
z[Y==2] <- 0.3
z[Y==3] <- 0.75
z[Y==4] <- 1.3
z[Y==5] <- 2
# lambda's
lambdas <- ss <- rep(NA,B)
lambdas[1] <- 1
alphas <- matrix(NA, B , length(coef(fit)) )
xtxi <- solve(t(X)%*%X)
g <- 1.4
set.seed(60532)
system.time(
for( i in 2:B) {
z_prev <- z
#sample gammas: annoying since indexing starts at 1, not 0
for( j in 2:4 ){
lb <- max(z[Y==j])
lb <- max(lb,gammas[i-1,as.character(j-1)])
ub <- min(z[Y==j+1])
ub <- min(ub,gammas[i-1,as.character(j+1)])
gammas[i,as.character(j)] <- runif( 1, min = lb, max = ub)
}
#sample Z's: no need to store and causes memory issues if we do, so not storing
for(j in 1:5){
z[Y==j] <- rtnorm(vec.n[j], X[Y==j,]%*%betas[i-1,], 1,
lower =  gammas[i-1,as.character(j-1)],
upper = gammas[i-1,as.character(j)])
}
#sample lambda
a = b = 1
ss[i] <- rinvgamma(1,a,b)
lambdas[i] <- 1/ss[i]
#sample alphas
alphas[i, ] <- rinvgauss(length(betas[i-1]), ((2*ss[i])/abs(betas[i-1])),  1)
#sample betas (MH)
betas[i,] <- rnorm(1 , 0 , sqrt( (4*ss[i])/alphas[i] ) )
}
)
setwd("G:\\math\\640")
rm(list=ls());gc()
library(MCMCpack)
library(mvtnorm)
library(MASS)
library(foreign)
library(msm)
library(data.table)
library(MCMCpack)
library(statmod)
# setwd("G:\\math\\640")
onp <- read.csv("onp_train.csv",header = T)
vec.remove <- c('X.1','X','url','timedelta','shares','ID','dt',
"n_non_stop_unique_tokens","n_non_stop_words","kw_avg_min","self_reference_max_shares",
"kw_min_min","self_reference_min_shares","kw_max_avg","rate_negative_words",
"avg_negative_polarity","rate_positive_words","title_subjectivity","max_positive_polarity",
"avg_positive_polarity","average_token_length","kw_max_min","global_rate_positive_words",
"kw_max_max")
onp <- onp[,!(colnames(onp) %in% vec.remove)]
## normalize, for penalty
# I know it's not truly noramlized, but since most vars are on a scale 0 to some high number
# or 0 to 1, this seemed close enough
onp$n_tokens_title <- onp$n_tokens_title/max(onp$n_tokens_title)
onp$n_tokens_content <- onp$n_tokens_content/max(onp$n_tokens_content)
onp$n_unique_tokens <- onp$n_unique_tokens/max(onp$n_unique_tokens)
onp$num_hrefs <- onp$num_hrefs/max(onp$num_hrefs)
onp$num_self_hrefs <- onp$num_self_hrefs/max(onp$num_self_hrefs)
onp$num_imgs <- onp$num_imgs/max(onp$num_imgs)
onp$num_videos <- onp$num_videos/max(onp$num_videos)
onp$num_keywords <- onp$num_keywords/max(onp$num_keywords)
onp$kw_min_max <- onp$kw_min_max/max(onp$kw_min_max)
onp$kw_avg_max <- onp$kw_avg_max/max(onp$kw_avg_max)
onp$kw_min_avg <- onp$kw_min_avg/max(onp$kw_min_avg)
onp$kw_avg_avg <- onp$kw_avg_avg/max(onp$kw_avg_avg)
onp$self_reference_avg_sharess <- onp$self_reference_avg_sharess/max(onp$self_reference_avg_sharess)
#### initialize variables
vec.keep <- c('n_tokens_content','num_hrefs','num_self_hrefs','num_imgs','num_videos','num_keywords','kw_min_max',
'kw_avg_max','kw_min_avg','kw_avg_avg','self_reference_avg_sharess','is_weekend','global_subjectivity',
'global_rate_negative_words','min_positive_polarity','title_sentiment_polarity','abs_title_subjectivity',
'abs_title_sentiment_polarity','topic')
sig.formula <- formula(paste0('as.factor(share_cat) ~ ',
paste0(vec.keep,collapse=' +')))
fit <- polr( sig.formula   , data = onp  , method = "probit" )
fit <- polr( as.factor(share_cat) ~ .   , data = onp  , method = "probit" )
X <- model.matrix(fit)
X <- X[,-1]
Y <- as.factor( onp[,'share_cat'] )
K <- ncol(X)
B <- 1000 # aiming for 4000, burn-in and thinning by 10
vec.n <- c(sum(Y==1),
sum(Y==2),
sum(Y==3),
sum(Y==4),
sum(Y==5))
# MLE Betas
dbetas <- function(beta,z,X,lambda){
exp(-0.5*t(z-X%*%beta)%*%(z-X%*%beta)-lambda*sum(abs(beta)))
}
logBetas <- function(beta,z,X,lambda){
-0.5*t(z-X%*%beta)%*%(z-X%*%beta)-lambda*sum(abs(beta))
}
betas <- matrix(NA, B , length(coef(fit)),
dimnames=list(1:B,names(coef(fit))))
beta <- coef(fit)
betas[1,] <- beta
beta.ar <- rep(0,B)
# MLE Gammas
gammas <- matrix( NA, B, 6, dimnames = list(1:B,0:5) )
gammas[,'0'] <- -Inf
gammas[,'1'] <- 0
gammas[,'5'] <- Inf
gammas[1, 3:5 ] <- fit$zeta[-1]
# Z's: initialize with midpoint of range
# now, don't plan to store z's, for memory
# Z <- matrix(NA, B , nrow(X))
z <- rep(NA,nrow(X))
z[Y==1] <- -0.5
z[Y==2] <- 0.3
z[Y==3] <- 0.75
z[Y==4] <- 1.3
z[Y==5] <- 2
# lambda's
lambdas <- ss <- rep(NA,B)
lambdas[1] <- 1
alphas <- matrix(NA, B , length(coef(fit)) )
xtxi <- solve(t(X)%*%X)
g <- 1.4
set.seed(60532)
system.time(
for( i in 2:B) {
z_prev <- z
#sample gammas: annoying since indexing starts at 1, not 0
for( j in 2:4 ){
lb <- max(z[Y==j])
lb <- max(lb,gammas[i-1,as.character(j-1)])
ub <- min(z[Y==j+1])
ub <- min(ub,gammas[i-1,as.character(j+1)])
gammas[i,as.character(j)] <- runif( 1, min = lb, max = ub)
}
#sample Z's: no need to store and causes memory issues if we do, so not storing
for(j in 1:5){
z[Y==j] <- rtnorm(vec.n[j], X[Y==j,]%*%betas[i-1,], 1,
lower =  gammas[i-1,as.character(j-1)],
upper = gammas[i-1,as.character(j)])
}
#sample lambda
a = b = 1
ss[i] <- rinvgamma(1,a,b)
lambdas[i] <- 1/ss[i]
#sample alphas
alphas[i, ] <- rinvgauss(length(betas[i-1]), ((2*ss[i])/abs(betas[i-1])),  1)
#sample betas (MH)
betas[i,] <- rnorm(1 , 0 , sqrt( (4*ss[i])/alphas[i] ) )
}
)
